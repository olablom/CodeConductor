name: CodeConductor Tests

on:
    pull_request:
        branches: [main, develop]
    push:
        branches: [main]

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: true

env:
    PYTHONHASHSEED: 0
    CODECONDUCTOR_SEED: 1337
    # Global CPU-lock environment variables
    CC_TESTING_MODE: "1"
    CC_GPU_DISABLED: "1"
    CC_FORCE_GPU: "0"
    CC_HARD_CPU_ONLY: "1"
    CUDA_VISIBLE_DEVICES: ""
    HF_HUB_OFFLINE: "1"
    TORCH_USE_CUDA_DISABLED: "1"
    VLLM_NO_CUDA: "1"
    # Block LM Studio during testing
    ENGINE_BACKENDS: "ollama"
    LMSTUDIO_DISABLE: "1"
    LMSTUDIO_CLI_DISABLE: "1"
    DISCOVERY_DISABLE: "1"
    ENGINE_DISCOVERY_MODE: "preloaded_only"

jobs:
    test:
        runs-on: ubuntu-latest

        strategy:
            matrix:
                python-version: [3.11]

        steps:
            - uses: actions/checkout@v4
              with:
                  ref: ${{ github.sha }}
                  fetch-depth: 0

            - name: Set up Python ${{ matrix.python-version }}
              uses: actions/setup-python@v4
              with:
                  python-version: ${{ matrix.python-version }}

            - name: Cache pip dependencies
              uses: actions/cache@v4
              with:
                  path: ~/.cache/pip
                  key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
                  restore-keys: |
                      ${{ runner.os }}-pip-

            - name: Install Python dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt
                  pip install pytest-json-report pytest-cov pytest-timeout

            - name: Install package in editable mode (with test extras)
              run: |
                  pip install -e .[test]

            - name: Run linting
              run: |
                  pip install black ruff
                  black --check src/
                  ruff check src/ --fix

            - name: Run CPU-locked unit tests with JSON report
              env:
                  PYTHONPATH: ${{ github.workspace }}/src
              run: |
                  mkdir -p .artifacts
                  python -m pytest tests/ -q -m "not gpu and not vllm" --json-report --json-report-file .artifacts/report.json

            - name: Run CPU-locked tests with coverage
              env:
                  PYTHONPATH: ${{ github.workspace }}/src
              run: |
                  python -m pytest -m "not gpu and not vllm" --cov=codeconductor --cov-config=.coveragerc --cov-report=term-missing --cov-report=html

            - name: Verify test results
              run: |
                  python - <<'PY'
                  import json, sys
                  try:
                      d = json.load(open(".artifacts/report.json", "r", encoding="utf-8"))
                      tests = d.get("tests", [])
                      passed = len([t for t in tests if t.get("outcome") == "passed"])
                      failed = len([t for t in tests if t.get("outcome") == "failed"])
                      skipped = len([t for t in tests if t.get("outcome") == "skipped"])
                      xfailed = len([t for t in tests if t.get("outcome") == "xfailed"])

                      print(f"Test Results: {passed} passed, {failed} failed, {skipped} skipped, {xfailed} xfailed")

                      if failed > 0:
                          print("❌ Tests failed!")
                          sys.exit(1)

                      print("✅ All tests passed!")

                  except Exception as e:
                      print(f"❌ Error reading test report: {e}")
                      sys.exit(1)
                  PY

            - name: Upload test report
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: test-report-linux
                  path: .artifacts/report.json

            - name: Upload coverage HTML
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: coverage-html-linux
                  path: htmlcov/

            - name: Upload test results
              uses: actions/upload-artifact@v4
              if: always()
              with:
                  name: test-results
                  path: |
                      .artifacts/
                      htmlcov/
                      *.log

    benchmark:
        runs-on: ubuntu-latest
        needs: test

        steps:
            - uses: actions/checkout@v4

            - name: Set up Python 3.11
              uses: actions/setup-python@v4
              with:
                  python-version: 3.11

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt

            - name: Run performance benchmarks
              run: |
                  if [ -f "tests/run_benchmarks.py" ]; then
                    python tests/run_benchmarks.py --agent-count=2 --quick
                  else
                    echo "⚠️ run_benchmarks.py not found, skipping benchmarks"
                  fi

            - name: Upload benchmark results
              uses: actions/upload-artifact@v4
              if: always()
              with:
                  name: benchmark-results
                  path: benchmark_results_*.json

name: CodeConductor Tests

on:
    pull_request:
        branches: [main, develop]
    push:
        branches: [main]

env:
    PYTHONHASHSEED: 0
    CODECONDUCTOR_SEED: 1337

jobs:
    test:
        runs-on: ubuntu-latest

        strategy:
            matrix:
                python-version: [3.11]

        steps:
            - uses: actions/checkout@v4

            - name: Set up Python ${{ matrix.python-version }}
              uses: actions/setup-python@v4
              with:
                  python-version: ${{ matrix.python-version }}

            - name: Cache pip dependencies
              uses: actions/cache@v4
              with:
                  path: ~/.cache/pip
                  key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
                  restore-keys: |
                      ${{ runner.os }}-pip-

            - name: Install Python dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt
                  pip install pytest-json-report pytest-cov pytest-timeout

            - name: Install package in editable mode
              run: |
                  pip install -e .

            - name: Run linting
              run: |
                  pip install black flake8
                  black --check src/
                  flake8 src/ --max-line-length=88

            - name: Run unit tests with JSON report
              env:
                  PYTHONPATH: ${{ github.workspace }}/src
                  CC_TESTING_MODE: 1
                  CC_GPU_DISABLED: 1
              run: |
                  mkdir -p .artifacts
                  python -m pytest tests/ -q --json-report --json-report-file .artifacts/report.json

            - name: Run tests with coverage
              env:
                  PYTHONPATH: ${{ github.workspace }}/src
                  CC_TESTING_MODE: 1
                  CC_GPU_DISABLED: 1
              run: |
                  python -m pytest --cov=codeconductor --cov-config=.coveragerc --cov-report=term-missing --cov-report=html

            - name: Verify test results
              run: |
                  python - <<'PY'
                  import json, sys
                  try:
                      d = json.load(open(".artifacts/report.json", "r", encoding="utf-8"))
                      tests = d.get("tests", [])
                      passed = len([t for t in tests if t.get("outcome") == "passed"])
                      failed = len([t for t in tests if t.get("outcome") == "failed"])
                      skipped = len([t for t in tests if t.get("outcome") == "skipped"])

                      print(f"Test Results: {passed} passed, {failed} failed, {skipped} skipped")

                      if failed > 0:
                          print("❌ Tests failed!")
                          sys.exit(1)

                      print("✅ All tests passed!")

                  except Exception as e:
                      print(f"❌ Error reading test report: {e}")
                      sys.exit(1)
                  PY

            - name: Upload test report
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: test-report-linux
                  path: .artifacts/report.json

            - name: Upload coverage HTML
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: coverage-html-linux
                  path: htmlcov/

            - name: Upload test results
              uses: actions/upload-artifact@v4
              if: always()
              with:
                  name: test-results
                  path: |
                      .artifacts/
                      htmlcov/
                      *.log

    benchmark:
        runs-on: ubuntu-latest
        needs: test

        steps:
            - uses: actions/checkout@v4

            - name: Set up Python 3.11
              uses: actions/setup-python@v4
              with:
                  python-version: 3.11

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt

            - name: Run performance benchmarks
              run: |
                  if [ -f "tests/run_benchmarks.py" ]; then
                    python tests/run_benchmarks.py --agent-count=2 --quick
                  else
                    echo "⚠️ run_benchmarks.py not found, skipping benchmarks"
                  fi

            - name: Upload benchmark results
              uses: actions/upload-artifact@v4
              if: always()
              with:
                  name: benchmark-results
                  path: benchmark_results_*.json
